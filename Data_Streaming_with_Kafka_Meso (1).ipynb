{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Streaming with Kafka**"
      ],
      "metadata": {
        "id": "mCGSShY3kU96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OVERVIEW**\n",
        "The project aims to build a Kafka pipeline that can receive real-time data from\n",
        "telecommunications mobile money transactions and process it for analysis. The pipeline should\n",
        "be designed to handle high volumes of data and ensure that the data is processed efficiently.\n"
      ],
      "metadata": {
        "id": "BlfLyU_WkZRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OBJECTIVES **\n",
        "1. Set up a Kafka cluster: You must set up a Kafka cluster that can handle high volumes\n",
        "of data. You can use either a cloud-based or on-premises Kafka cluster.\n",
        "2. Develop a Kafka producer: You must develop a Kafka producer that can ingest data\n",
        "from telecommunications mobile money transactions and send it to the Kafka cluster.\n",
        "The producer should be designed to handle high volumes of data and ensure that the\n",
        "data is sent to the Kafka cluster efficiently.\n",
        "3. Develop a Kafka consumer: You must develop a Kafka consumer to receive data from\n",
        "the Kafka cluster and process it for analysis. The consumer should be designed to\n",
        "handle high volumes of data and ensure that the data is processed efficiently.\n",
        "4. Process the data: Once you have set up the Kafka pipeline, you must process the data\n",
        "for analysis. This may involve cleaning and aggregating the data, performing\n",
        "calculations, and creating visualizations.\n",
        "5. Test the solution: You must test your solution using the provided dummy json file. The\n",
        "file contains sample data that you can use to ensure that your Kafka pipeline is working\n",
        "correctly.\n",
        "Hereâ€™s the dummy JSON file that represents"
      ],
      "metadata": {
        "id": "NlPpKxYDkldJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s36ar2kFfcd5",
        "outputId": "3ef044f7-9885-45f9-c698-26259969a42b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: confluent-kafka in /usr/local/lib/python3.10/dist-packages (2.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install confluent-kafka"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kafka-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoX2zGpczrgf",
        "outputId": "7e1f269a-a511-4d63-8ebf-c305a0287367"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.10/dist-packages (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kafka cluster setup"
      ],
      "metadata": {
        "id": "JX1dcyLJTVsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Kafka Producer instance and configure it with your Confluent Cloud credentials:"
      ],
      "metadata": {
        "id": "NPxKzcBetNu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Producer, KafkaError\n",
        "\n",
        "import random\n",
        "import time\n",
        "# Set up Confluent Cloud credentials\n",
        "conf = {\n",
        "'bootstrap.servers': 'pkc-ewzgj.europe-west4.gcp.confluent.cloud:9092',\n",
        "'security.protocol': 'SASL_SSL',\n",
        "'sasl.mechanism': 'PLAIN',\n",
        "'sasl.username': 'JWDRXK7ZNET3SSQO',\n",
        "'sasl.password': 'eRZ18nXGN9kAxvflMUZU4wwBQhFWM+P5MRsgFncVDV3P49IUbz4mNwGh3e4ho47M', }"
      ],
      "metadata": {
        "id": "HDaJJsDytM5h"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test\n",
        "# Create a Kafka producer instance\n",
        "producer = Producer(conf)\n",
        "# Publish dummy data to the topic every second\n",
        "topic_name = 'moringa'\n",
        "\n",
        "while True:\n",
        "# Generate a random number between 0 and 100 as the message\n",
        " message = str(random.randint(0, 100))\n",
        " producer.produce(topic_name, message.encode('utf-8'))\n",
        " producer.flush()\n",
        "\n",
        "print(f\"Published message '{message}' to topic '{topic_name}'\")\n",
        "time.sleep(1)"
      ],
      "metadata": {
        "id": "_H-1BAXXfmmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Kafka Consumer instance and configure it with your Confluent Cloud credentials:"
      ],
      "metadata": {
        "id": "FuQOC21Gt7__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Consumer, KafkaError\n",
        "\n",
        "# Set up Confluent Cloud credentials\n",
        "conf = {\n",
        "'bootstrap.servers': 'pkc-ewzgj.europe-west4.gcp.confluent.cloud:9092',\n",
        "'security.protocol': 'SASL_SSL',\n",
        "'sasl.mechanism': 'PLAIN',\n",
        "'sasl.username': 'JWDRXK7ZNET3SSQO',\n",
        "'sasl.password': 'eRZ18nXGN9kAxvflMUZU4wwBQhFWM+P5MRsgFncVDV3P49IUbz4mNwGh3e4ho47M', \n",
        "'group.id': 'learning'\n",
        "}"
      ],
      "metadata": {
        "id": "51vMmvtauNF3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "# Create a Kafka consumer instance\n",
        "consumer = Consumer(conf)\n",
        "topic_name = 'moringa'\n",
        "\n",
        "# Subscribe to your Confluent Cloud topic and consume messages\n",
        "consumer.subscribe([topic_name])\n",
        "while True:\n",
        " msg = consumer.poll(1.0)\n",
        "if msg is None:\n",
        "continue\n",
        "if msg.error():\n",
        "print(\"Consumer error: {}\".format(msg.error()))\n",
        "continue\n",
        "print('Received message: {}'.format(msg.value().decode('utf-8')))"
      ],
      "metadata": {
        "id": "YIizbmn6ug7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writing Data to Kafka Topics"
      ],
      "metadata": {
        "id": "lVFMP_QNw80h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kafka import KafkaProducer\n",
        "import json\n",
        "\n",
        "# Create Kafka producer\n",
        "producer = KafkaProducer(bootstrap_servers=['pkc-ewzgj.europe-west4.gcp.confluent.cloud:9092'])\n",
        "\n",
        "# Define sample CDR data\n",
        "data = {\"transaction_id\": \"12345\",\n",
        "\"sender_phone_number\": \"256777123456\",\n",
        "\"receiver_phone_number\": \"256772987654\",\n",
        "\"transaction_amount\": 100000,\n",
        "\"transaction_time\": \"2023-04-19 12:00:00\"\n",
        "}\n",
        "\n",
        "# Serialize CDR data to JSON\n",
        "serialized_cdr = json.dumps(data).encode('utf-8')\n",
        "\n",
        "# Produce CDRs to Kafka topic\n",
        "producer.send('moringa', value=serialized_cdr)"
      ],
      "metadata": {
        "id": "8esbTiAyPCV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Consumer, KafkaError\n",
        "\n",
        "consumer_conf = {\n",
        "    'bootstrap.servers': 'pkc-ewzgj.europe-west4.gcp.confluent.cloud:9092',\n",
        "    'group.id': 'my-group',\n",
        "    'auto.offset.reset': 'earliest',\n",
        "    'enable.auto.commit': False  # Disable auto commit\n",
        "}\n",
        "\n",
        "consumer = Consumer(consumer_conf)\n",
        "\n",
        "while True:\n",
        "    msg = consumer.poll(1.0)\n",
        "    if msg is None:\n",
        "        continue\n",
        "    if msg.error():\n",
        "        if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "            print(f'Reached end of partition {msg.topic()}-{msg.partition()}')\n",
        "        else:\n",
        "            print(f'Error while consuming messages: {msg.error()}')\n",
        "    else:\n",
        "        print(f'Received message: {msg.value()}')\n",
        "        consumer.commit({\n",
        "            'topic': msg.topic(),\n",
        "            'partition': msg.partition(),\n",
        "            'offset': msg.offset()\n",
        "        })"
      ],
      "metadata": {
        "id": "kZzTYw3nQqYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data processing and analysis"
      ],
      "metadata": {
        "id": "p-R0heHCSSPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Consumer, Producer, KafkaError\n",
        "import json\n",
        "\n",
        "# Set up Kafka consumer\n",
        "consumer = Consumer({\n",
        "    'bootstrap.servers': 'pkc-ewzgj.europe-west4.gcp.confluent.cloud:9092',\n",
        "    'group.id': 'group1',\n",
        "    'auto.offset.reset': 'earliest'\n",
        "})\n",
        "consumer.subscribe(['mobile_money_transactions'])\n",
        "\n",
        "# Set up Kafka producer\n",
        "producer = Producer({'bootstrap.servers': 'pkc-ewzgj.europe-west4.gcp.confluent.cloud:9092'})\n",
        "\n",
        "# Define function to filter data\n",
        "def filter_data(msg):\n",
        "    data = json.loads(msg.value().decode('utf-8'))\n",
        "    if data['region'] == 'Kampala':\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Start streaming data and filter transactions\n",
        "while True:\n",
        "    msg = consumer.poll(1.0)\n",
        "    if msg is None:\n",
        "        continue\n",
        "    if msg.error():\n",
        "        print(\"Consumer error: {}\".format(msg.error()))\n",
        "        continue\n",
        "    if filter_data(msg):\n",
        "        producer.produce('kampala_transactions', msg.value())\n",
        "    producer.flush()"
      ],
      "metadata": {
        "id": "64MSfnR3SdvD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}